With a similar state of mind, we believe that evaluation of transfer learning systems applied to anomaly detection cannot rely solely on classic metrics such as AUC and ERR; if we seek to measure how well a system trained within one domain's feature space performs on a dissimilar domain's space, the idea of generalization presents itself as a great fit. We propose then to adapt the idea in the form of two metrics: (i) Patial Cross-domain Feature Space Generalization (Partial CDFG) and (ii) Complete Cross-domain Feature Space Generalization (Complete CDFG), defined as:

\begin{equation}
    G_{part}(f_n^A) = |\underset{x \in \mathcal{X}^A}{R(f_n^A)} - \underset{x \in \mathcal{X}^B}{R(f_n^A)}|
\end{equation}

\begin{equation}
    G_{comp}(f_n^A, f_n^B) = \dfrac{1}{2}(|\underset{x \in \mathcal{X}^A}{R(f_n^A)} - \underset{x \in \mathcal{X}^B}{R(f_n^A)}| + |\underset{x \in \mathcal{X}^B}{R(f_n^B)} - \underset{x \in \mathcal{X}^A}{R(f_n^B)}|)
\end{equation}

Where $f_n^A$ and $f_n^B$ are classifiers found by a classification algorithm by the way of Empirical Risk Minimization applied over, respectively, domain $A$ and domain $B$; the expression $\underset{x \in \mathcal{X}^A}{R(f_n^A)}$ denotes the risk of classifier $f_n^A$ over the feature space $\mathcal{X}^A$ and $\underset{x \in \mathcal{X}^B}{R(f_n^A)}$ denotes the risk of the same classifier $f_n^A$ over the feature space of the second domain, $\mathcal{X}^B$; the mirrored definition being valid for $\underset{x \in \mathcal{X}^B}{R(f_n^B)}$ and $\underset{x \in \mathcal{X}^A}{R(f_n^B)}$

Even though the real risk is intractable, it could be approximated by test sets (data that is specially separated to represent unseen data) from each domain; Finally, transferring those idea to the context of anomaly detection, we could take risk in this formulation as EER or the $1-AUC$, both good and classic metrics of the field.

Beside the formulation of the metrics, we also seek to guarantee their significance and meaningfulness by restricting the scenarios upon which it could be applied. 

Given two domains $A$ and $B$ and their respective feature spaces $\mathcal{X}^A$ and $\mathcal{X}^B$, Partial and Complete CDFG are meaningful metrics if: 

\begin{itemize}
    \item The bias (set of possible classifiers) of the classification/detection algorithm in use must be the same (ex.: same parameters on an SVM training setup);
    \item The spaces $\mathcal{X}^A$ and $\mathcal{X}^B$ are composed of the same set of descriptors for both domains;
    \item The transfer learning or domain mapping method should have no prior knowledge of test data on either domain
\end{itemize}


%%%%

Evaluating with CDFG affords us a multi-leveled analysis of feature spaces and transfer learning sytems. \textit{Partial CDFG} is a good representation of how well training over a domain $A$ is well-suited or adapted to work over samples from domain $B$ with consistent performance. It is however 'one-way' regarding domain that is, we are looking at the mapping or transfering characteristic only in the $A \longrightarrow B$ direction. It is, regardless of this limitation, relavant to the analysis, specially when used in conjunction with \textit{Complete CDFG}. While \textit{Partial CDFG} is partial to the chosen descriptors and how each domain is particularly well represented by such descriptors, \textit{Complete CDFG} is a best measure of the quality of the transfer system itself and it's robustness when tested over different contexts and pairs of domains


%%%%%%

We are going to hereafter introduce three particular analysis levels afforded by our novel metrics. We are going to compare methodologies (defined as the pair of classification algorithm and transfer learning technique), two at a time, and indicate those by the subscripts $\alpha$ and $\beta$, where selected classifiers obtained through the principle of empirical risk minimization for each methodology will be denoted as $f_\alpha$ and $f_\beta$. One can say that a method is more \textit{generalizable} in different levels by satisfying inequalities, one of such expressed bellow:

\begin{equation}    
\label{eq:OneDirectionA}
G_{part}(f_\alpha^A) < G_{part}(f_\beta^A)
\end{equation}

With this relationship satisfied, one could say method $\alpha$ is capable of generalizing well from domain $A$ to domain $B$. One can also look at the Partial CDFG metric from the ``opposite direction'' and assess if the $\alpha$ methodology is also better than $\beta$ at generalizing from $B$ to $A$, as expressed by the inequality:

\begin{equation} 
\label{eq:OneDirectionB}
G_{part}(f_\alpha^B) < G_{part}(f_\beta^B)
\end{equation}

So, evaluating the Partial CDFG on both directions gives us an understanding of the first level of generalization: how well the space obtained from one domain is applicable to another and how this applicability is captured by the chosen methodology. These comparisons do not assess directly the transfer method, being influenced and capturing well aspects regarding the representability of each feature space. To obtain a more precise and rigorous analysis of the transfer learning method in itself, we should compare using the Complete CDFG metric:

\begin{equation} 
\label{eq:TwoDirections}
G_{comp}(f_\alpha^A, f_\alpha^B) < G_{comp}(f_\beta^A, f_\beta^B)
\end{equation}

We highlight however that the best use of our metrics come from looking at each Partial and the Complete CDFG at the same time, given that the Complete CDFG can be influenced by high discrepancy between the Partials that compose it. It is primal then that all three comparisons are taken into account in the assessment of any two competing methods.


A MLP tem formulação semelhante àquela do Perceptron, porém com uma camada escondida entre as camadas de entrada e de saída. Chamemos os padrões de entrada de x_i, e cada escalar de x_ij. 

Em geral o perceptron se conecta da seguinte forma (generalizado):

x_ij --w_lj--> f(net^h_il) --w_kl--> f(net^o_ik) --> output

net^o_ik = \sum_l{f(net^h_il).w^o_kl} + b^o_k
net^h_il = \sum_k{x_ij*w^h_lj} + b^h_l
f(x) = 1/(1+exp(-x))
\hat{y} = f(net^o_ik)

A função de custo comumente utilizada para a MLP é a diferença entre o esperado e o produzido, ao quadrado:

E^2 = (y - \hat{y})^2 

Dessa forma, ao aplicar gradiente descentente (em sua forma generalizada, backpropagation/generalized delta rule) temos melhroes garantias de que há um ponto de mínimo. Este ponto de mínimo é complicado pelo uso da função f(.) sigmoidal; sabemos porém que f(.)^2 é uma função semiconvexa e há teoremas que garantem que E^2 também será.

A atualização dos pesos é realizada através de backpropagation:

w^o_kl(t+1) = w^o_kl(t) - \eta.dE^2/dw^o_kl
dE^2/dw^o_kl = d(y - \hat{y})^2/dw^o_kl = 2(y - \hat{y})(df(net^o_ik)/dw^o_kl) = 
2(y - \hat{y})f(net^o_ik).(1-f(net^o_ik)).(dnet^o_ik/dw^o_kl) = 
2(y - \hat{y})f(net^o_ik).(1-f(net^o_ik)).f(net^h_il)

w^h_lj(t+1) = w^h_lj(t) - \eta.dE^2/dw^h_lj
dE^2/dw^h_lj = 2(y - \hat{y})(df(\sum_k{net^o_ik)}/dw^h_lj) = 
dE^2/dw^h_lj = 2(y - \hat{y}).\sum_k(1-f(net^o_ik)).f(net^h_il)

...